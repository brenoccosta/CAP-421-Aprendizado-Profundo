{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c07f06b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import xarray\n",
    "import rasterio\n",
    "import torch.nn as nn\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.transforms import v2 as transforms\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "data_cubes_dir = \"./Data/Images/Data Cubes\"\n",
    "train_dir = f\"{data_cubes_dir}/Train\"\n",
    "val_dir = f\"{data_cubes_dir}/Val\"\n",
    "test_dir = f\"{data_cubes_dir}/Test\"\n",
    "train_cubes_paths = glob(f\"{train_dir}/*.zarr\")\n",
    "val_cubes_paths = glob(f\"{val_dir}/*.zarr\")\n",
    "test_cubes_paths = glob(f\"{test_dir}/*.zarr\")\n",
    "all_cubes_paths = train_cubes_paths + val_cubes_paths + test_cubes_paths\n",
    "\n",
    "\n",
    "class AtlanticForestDataset(Dataset):\n",
    "  def __init__(self, zarr_paths, transform=None):\n",
    "    self.zarr_paths = zarr_paths\n",
    "    self.transform = transform\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.zarr_paths)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    zarr_path = self.zarr_paths[idx]\n",
    "    data_cube = xarray.open_zarr(zarr_path, consolidated=False)['data_cube']\n",
    "    tensor = torch.from_numpy(data_cube.values).float()\n",
    "    tensor = torch.nan_to_num(tensor)\n",
    "    if self.transform:\n",
    "      tensor = torch.transpose(tensor, 0, 1)\n",
    "      tensor = self.transform(tensor)\n",
    "      tensor = torch.transpose(tensor, 0, 1)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "# Using the mean and std from the train dataset (check AutoEncoder Train for more details)\n",
    "mean = torch.tensor([881.1200, 996.9095, 839.9584, 3211.0962, 1839.4138, 5219.2334, 2899.2646])\n",
    "std = torch.tensor([1692.1893, 1562.1083, 1555.0720, 1280.2616, 1096.6338, 2067.6047, 1164.6179])\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "  transforms.Normalize(mean=mean.tolist(), std=std.tolist())\n",
    "])\n",
    "\n",
    "\n",
    "all_dataset = AtlanticForestDataset(all_cubes_paths, transform=transform)\n",
    "all_dataloader = DataLoader(all_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f43854b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AutoEncoder(\n",
       "  (encoder): Sequential(\n",
       "    (0): Conv3d(7, 5, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(0, 1, 1))\n",
       "    (1): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Conv3d(5, 5, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (5): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): Conv3d(5, 3, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (9): BatchNorm3d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): Conv3d(3, 1, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (13): BatchNorm3d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (decoder): Sequential(\n",
       "    (0): ConvTranspose3d(1, 3, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (1): BatchNorm3d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): LeakyReLU(negative_slope=0.01)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): ConvTranspose3d(3, 5, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (5): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.01)\n",
       "    (7): Dropout(p=0.2, inplace=False)\n",
       "    (8): ConvTranspose3d(5, 5, kernel_size=(5, 3, 3), stride=(1, 1, 1), padding=(0, 1, 1))\n",
       "    (9): BatchNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.01)\n",
       "    (11): Dropout(p=0.2, inplace=False)\n",
       "    (12): ConvTranspose3d(5, 7, kernel_size=(3, 3, 3), stride=(2, 1, 1), padding=(0, 1, 1))\n",
       "    (13): BatchNorm3d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(AutoEncoder, self).__init__()\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Conv3d(\n",
    "        in_channels=7, \n",
    "        out_channels=5, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        stride=(2, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(5),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.Conv3d(\n",
    "        in_channels=5, \n",
    "        out_channels=5, \n",
    "        kernel_size=(5, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(5),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.Conv3d(\n",
    "        in_channels=5, \n",
    "        out_channels=3, \n",
    "        kernel_size=(5, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(3),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.Conv3d(\n",
    "        in_channels=3, \n",
    "        out_channels=1, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(1)\n",
    "    )\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.ConvTranspose3d(\n",
    "        in_channels=1, \n",
    "        out_channels=3, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(3),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.ConvTranspose3d(\n",
    "        in_channels=3, \n",
    "        out_channels=5, \n",
    "        kernel_size=(5, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(5),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.ConvTranspose3d(\n",
    "        in_channels=5, \n",
    "        out_channels=5, \n",
    "        kernel_size=(5, 3, 3),\n",
    "        stride=(1, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(5),\n",
    "      nn.LeakyReLU(),\n",
    "      nn.Dropout(0.2),\n",
    "      nn.ConvTranspose3d(\n",
    "        in_channels=5, \n",
    "        out_channels=7, \n",
    "        kernel_size=(3, 3, 3),\n",
    "        stride=(2, 1, 1),\n",
    "        padding=(0, 1, 1)),\n",
    "      nn.BatchNorm3d(7)\n",
    "    )  \n",
    "\n",
    "  def encode(self, x):\n",
    "    return self.encoder(x)\n",
    "  \n",
    "  def decode(self, x):\n",
    "    return self.decoder(x)\n",
    "\n",
    "  def forward(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "model = AutoEncoder()\n",
    "model_path = \"./Model/AutoEncoder 10.pth\"\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d322c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Writing patches [....................................................................................................]\n"
     ]
    }
   ],
   "source": [
    "print(f\"[INFO] Writing patches [\", end=\"\")\n",
    "\n",
    "pattern = re.compile(r\"Data_Cube_(\\d+)\\.zarr\")\n",
    "\n",
    "for i, batch in enumerate(all_dataloader):\n",
    "  encoded_batch = model.encode(batch)\n",
    "  encoded_batch = torch.squeeze(encoded_batch)\n",
    "  encoded_batch = encoded_batch.detach().numpy()\n",
    "  \n",
    "  for j, latent in enumerate(encoded_batch):  \n",
    "    if not (i * batch_size + j + 1) % (len(all_cubes_paths) // 100): print(\".\", end=\"\")\n",
    "  \n",
    "    cube_path = all_cubes_paths[i * batch_size + j]\n",
    "    cube_id = int(pattern.search(cube_path).group(1))\n",
    "    cube = xarray.open_zarr(cube_path, consolidated=False)\n",
    "    out_meta = {\n",
    "      'driver': 'GTiff',\n",
    "      'dtype': \"float32\",\n",
    "      'nodata': -9999.0,\n",
    "      'width': 129,\n",
    "      'height': 129,\n",
    "      'count': 1,\n",
    "      'crs': cube.spatial_ref.crs_wkt,\n",
    "      'transform': cube.rio.transform()\n",
    "    }\n",
    "\n",
    "    output_folder = f\"./Data/Images/Latent Patches\"\n",
    "    output_file = f\"{output_folder}/Latent_Patch_{cube_id}.tif\"\n",
    "    with rasterio.open(output_file, \"w\", **out_meta) as dst:\n",
    "      dst.write(latent, 1)\n",
    "  \n",
    "print(\"]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
